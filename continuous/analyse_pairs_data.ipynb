{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:26:53.772862Z",
     "start_time": "2024-05-22T09:26:52.033768Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import ind_pl as ind\n",
    "from pprint import pprint\n",
    "from continuous import components\n",
    "import seaborn as sns\n",
    "from binance import Client\n",
    "from mt.resources import keys\n",
    "client = Client(keys.mPkey, keys.mSkey)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def choose_by_length(minimum: int|str=27500, maximum:int|str=420000):\n",
    "    \"\"\"checks the length of ohlc history of each trading pair, then makes a list of all pairs whos history length falls\n",
    "    within the stated range\"\"\"\n",
    "\n",
    "    lengths = {'1 month': 8750, '2 months': 17500, '3 months': 26250, '6 months': 52500,\n",
    "               '1 year': 105000, '2 years': 210000, '3 years': 315000, '4 years': 420000}\n",
    "\n",
    "    if isinstance(minimum, str):\n",
    "        minimum = lengths[minimum]\n",
    "    if isinstance(maximum, str):\n",
    "        maximum = lengths[maximum]\n",
    "\n",
    "    info = {}\n",
    "    data_path = Path(\"/home/ross/coding/modular_trader/bin_ohlc_5m\")\n",
    "    for pair_path in list(data_path.glob('*')):\n",
    "        df = pl.read_parquet(pair_path)\n",
    "        info[pair_path.stem] = len(df)\n",
    "\n",
    "    return [p for p, v in info.items() if minimum < v <= maximum]\n",
    "\n",
    "def resample(df, timeframe):\n",
    "    df = df.sort('timestamp').set_sorted('timestamp')\n",
    "    \n",
    "    df = (df.group_by_dynamic(pl.col('timestamp'), every=timeframe).agg(\n",
    "        pl.first('open'),\n",
    "        pl.max('high'),\n",
    "        pl.min('low'),\n",
    "        pl.last('close'),\n",
    "        pl.sum('base_vol'),\n",
    "        pl.sum('quote_vol'),\n",
    "        pl.sum('num_trades'),\n",
    "        pl.sum('taker_buy_base_vol'),\n",
    "        pl.sum('taker_buy_quote_vol'),\n",
    "        ))\n",
    "\n",
    "    df = df.sort('timestamp')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def top_heavy(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"calculates my 'top heavy' metric, which is a ratio representing the balance of total historic\n",
    "    volume below the current price relative to total historic volume above the current price\"\"\"\n",
    "    \n",
    "    current_price = df.item(-1, 'close')\n",
    "    \n",
    "    above = df.filter(pl.col('close') > current_price)['base_vol'].sum()\n",
    "    below = df.filter(pl.col('close') < current_price)['base_vol'].sum()\n",
    "    \n",
    "    return above, below, above / below\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-22T09:26:53.843571Z",
     "start_time": "2024-05-22T09:26:53.774939Z"
    }
   },
   "id": "6cbdf0e6e83ba1e2",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# top_heavy refers to an analysis i just thought up: is there more historic volume above or below current price? \n",
    "# this should be calculated as total volume above current price / total volume below\n",
    "\n",
    "# to analyse the effect of daily/weekly rsi on a pair's suitablility for selection, i could go through ohlc history calculating rolling sharpe alongside rsi, then calculate the forward-looking difference between rsi values and rsi values 1 week/month ahead (so the diff in each period is a projection of how the rsi will change over the coming week/month), then compare each diff to it's corresponding sharpe value to see if there is a 'good' range of rsi values to select for (is it better to select overbought pairs, oversold pairs, or something else). it might even turn out to be a combination of daily and weekly that is most useful, like if one is high and the other is low then good, but if both high or both low then bad\n",
    "\n",
    "# it seems that the shortest history that works with the ichi trend strategy is 27500 5min periods"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T11:27:34.852418Z",
     "start_time": "2024-05-14T11:27:34.844362Z"
    }
   },
   "id": "351b366de1cc9853",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "idea: an allocator function that chooses what proportion of capital to allocate to trend-following vs mean-reversion. calculate a price channel based on a rolling window, then analyse where in the channel price spends most of the time. my guess is during trending regimes price will mostly be at the edges of the channel, and during ranging conditions price will spend most of the time in the middle of the channel. \n",
    "so an indicator could be created by calculating the distance from the middle of the channel to the (smoothed) current price, so if price is close to the middle, more weight is given to the mean-reverting forecasts, and if price is out at the extremes, more weight is given to the trend-following forecasts. \n",
    "\n",
    "i could also investigate whether vwma can tell me anything useful here, like if price is at the edge of the channel and 25h vwma is further out than 25h ema then continuation is more likely, but if ema is further out than vwma then mean-reversion is more likely etc. i could investigate this same idea with rsi too, ie when rsi is t the extremes, can the relationship between vwma and a non-volume ma have any predictive power about continuation or reversion? \n",
    "\n",
    "this will probably hurt overall profitability but might improve sharpe ratio.\n",
    "\n",
    "i could even use machine learning here maybe, by looking at the channel position at time t, t-1, t-2, t-3 etc, can i get a solid probability on t+1's price action being either continuation or reversal? could vwma/ema ratio be a useful feature here? or volume delta? or volume z-score? or num_trades z-score?"
   ],
   "id": "a20a19a5077c407d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T16:34:02.858936Z",
     "start_time": "2024-05-27T16:26:16.485647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_info = client.get_exchange_info()\n",
    "tick_sizes = {x['symbol']: float(x['filters'][0]['tickSize']) for x in x_info['symbols']}\n",
    "\n",
    "info = {\n",
    "    'pair': [],\n",
    "    'length': [],\n",
    "    'daily_volume': [],\n",
    "    'weekly_volume': [],\n",
    "    'daily_volume_change': [],\n",
    "    'daily_atr': [],\n",
    "    'tick_size': [],\n",
    "    # 'weekly_rsi': [],\n",
    "    # 'monthly_rsi': [],\n",
    "    'volume_above_pw': [],  # historic volume above the current price, in multiples of current weekly volume\n",
    "    'volume_below_pw': [],  # historic volume below the current price, in multiples of current weekly volume\n",
    "    'top_heavy': [],\n",
    "    # 'sharpe': [],  # this will have to be backtested\n",
    "    # 'mcap': [],  # this will have to come from coingecko\n",
    "    'divisibility': []\n",
    "}\n",
    "\n",
    "pairs = choose_by_length()\n",
    "\n",
    "for pair in pairs:\n",
    "    pair_path = Path(f\"/home/ross/coding/pi_3/modular_trader/bin_ohlc_5m/{pair}.parquet\")\n",
    "    try:\n",
    "        df = pl.read_parquet(pair_path)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    weekly_df = resample(df, '1w')\n",
    "    monthly_df = resample(df, '1m')\n",
    "    one_day_volume = df['quote_vol'].tail(288).sum()\n",
    "    seven_day_volume = df['quote_vol'].tail(2016).sum()\n",
    "    daily_volume_change = df['quote_vol'].ewm_mean(span=288).pct_change(288)[-1]\n",
    "    daily_atr_pct = ind.atr(df, 48, 288, 'pct').item(-1, 'atr_48_288_pct')\n",
    "    daily_atr_abs = ind.atr(df, 48, 288, 'abs').item(-1, 'atr_48_288_abs')\n",
    "    info['pair'].append(pair)\n",
    "    info['length'].append(len(df))\n",
    "    info['daily_volume'].append(one_day_volume)\n",
    "    info['weekly_volume'].append(seven_day_volume)\n",
    "    info['daily_volume_change'].append(daily_volume_change)\n",
    "    info['daily_atr'].append(daily_atr_pct)\n",
    "    info['tick_size'].append(tick_sizes[pair])\n",
    "    # info['weekly_rsi'].append(weekly_df['close'])\n",
    "    # info['monthly_rsi'].append(monthly_df['close'])\n",
    "    info['volume_above_pw'].append(top_heavy(df)[0] / seven_day_volume)\n",
    "    info['volume_below_pw'].append(top_heavy(df)[1] / seven_day_volume)\n",
    "    info['top_heavy'].append(top_heavy(df)[2])\n",
    "    info['divisibility'].append(daily_atr_abs / tick_sizes[pair])\n",
    "\n",
    "info_df = pl.from_dict(info)\n",
    "lively_pairs = info_df.filter(\n",
    "    pl.col('divisibility').gt(80.0),\n",
    "    pl.col('daily_volume').gt(2_500_000),\n",
    "    pl.col('length').gt(27_500),\n",
    "    pl.col('daily_atr').rank(descending=True).lt(50),\n",
    ")\n",
    "\n",
    "# load ohlc, resample to 1h, then calculate correlation matrix for all pairs in lively pairs, and record avg correlation as a new stat for each pair\n",
    "all_closes = {}\n",
    "for pair in lively_pairs['pair']:\n",
    "    pair_path = Path(f\"/home/ross/coding/pi_3/modular_trader/bin_ohlc_5m/{pair}.parquet\")\n",
    "    try:\n",
    "        df = pl.read_parquet(pair_path)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    df = resample(df, '1h')\n",
    "    all_closes[pair] = df['close']\n",
    "\n",
    "min_length = min([len(x) for x in all_closes.values()])\n",
    "all_closes = {k: v.tail(min_length) for k, v in all_closes.items()}\n",
    "closes_df = pl.DataFrame(all_closes)\n",
    "(\n",
    "    lively_pairs\n",
    "    .with_columns(\n",
    "        pl.Series(closes_df.corr()\n",
    "                  .mean()\n",
    "                  .to_dicts()[0]\n",
    "                  .values()\n",
    "                  )\n",
    "        .alias('avg_correlation')\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col('avg_correlation')\n",
    "        .rank()\n",
    "        .lt(16)\n",
    "    )\n",
    "    ['pair']\n",
    "    .to_list()\n",
    ")"
   ],
   "id": "1a35db4f8a85dab8",
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dead_pairs = info_df.filter(\n",
    "    pl.col('volume_above_pw').rank(descending=True) < 20,  # select the 10 pairs with the highest volume_above\n",
    "    pl.col('top_heavy') > 1\n",
    ")\n",
    "\n",
    "likely_shorts = info_df.filter(\n",
    "    pl.col('volume_above_pw').rank(descending=True) < 20,  # select the 10 pairs with the highest volume_above\n",
    "    pl.col('top_heavy') < 1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-10T10:47:08.564245Z",
     "start_time": "2024-05-10T10:47:08.559611Z"
    }
   },
   "id": "701bd7a9afc08970",
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dead_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T12:49:31.090460Z",
     "start_time": "2024-03-20T12:49:31.085691Z"
    }
   },
   "id": "e3bf19186d41abb9",
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "likely_shorts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T12:50:03.080743Z",
     "start_time": "2024-03-20T12:50:03.076212Z"
    }
   },
   "id": "741826b7283b98d5",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T10:51:25.379299Z",
     "start_time": "2024-05-13T10:51:25.374390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from binance import Client\n",
    "from mt.resources import keys\n",
    "client = Client(keys.mPkey, keys.mSkey)\n",
    "\n",
    "info = client.get_exchange_info()\n",
    "tick_sizes = {x['symbol']: float(x['filters'][0]['tickSize']) for x in info['symbols']}"
   ],
   "id": "a3a29756e02b5f80",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T10:59:07.410095Z",
     "start_time": "2024-05-13T10:59:06.429643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tickers = client.get_ticker()\n",
    "prices = {x['symbol']: float(x['weightedAvgPrice']) for x in tickers}"
   ],
   "id": "c2a51b1e3289419a",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T11:04:30.932322Z",
     "start_time": "2024-05-13T11:04:30.919648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "divisibility = {}\n",
    "for p in lively_pairs['pair']:\n",
    "    divisibility.update({p: (prices[p] / tick_sizes[p])})\n",
    "\n",
    "divisibility"
   ],
   "id": "225f38d4e4f84ed",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T10:02:27.608724Z",
     "start_time": "2024-05-14T10:02:27.380605Z"
    }
   },
   "cell_type": "code",
   "source": "tickers",
   "id": "fdf1f27cde630b3b",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T11:04:09.569740Z",
     "start_time": "2024-05-14T11:04:07.968426Z"
    }
   },
   "cell_type": "code",
   "source": "client.get_symbol_info('1000SATSUSDT')",
   "id": "8ea4760f272d2340",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "747517f633d981ac",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
